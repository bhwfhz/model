# train_resampled.py
"""
Multi-step training script for resampled dataset.
Assumes processed_resampled_events/*.npz exist, each containing:
  - X: (T_in, 3)  # axes order: z, y, x (as we prepared)
  - Y: (T_out, 9, 3)

This script generates (X_window, Y_future) training pairs:
  - X_window shape: (window_size, in_H, in_W, in_C)
  - Y_future shape: (out_steps, out_H, out_W, out_C)

Configuration near the top — change target_fs/window_seconds/out_seconds as needed.
"""
import os
import glob
import json
import pickle
import time
import numpy as np
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau

from model import build_hybrid_conv_lstm_seq2seq

# reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# ---------------- Config (modify if needed) ----------------
processed_dir = "./processed_resampled_events"   # generated by data_loader_resample.py
if not os.path.isdir(processed_dir):
    raise RuntimeError(f"请先运行 data_loader_resample.py，生成 {processed_dir}")

target_fs = 50.0          # must match preprocessing resample target
window_seconds = 16.0
out_seconds = 60.0

window_size = int(round(window_seconds * target_fs))   # e.g., 800
out_steps = int(round(out_seconds * target_fs))        # e.g., 3000
stride_seconds = 1.0
stride = max(1, int(round(stride_seconds * target_fs)))   # e.g., 50 -> slide by 1s, adjust as needed

# Input spatialization: treat 3 axes as spatial height
in_H, in_W, in_C = 3, 1, 1
out_H, out_W, out_C = 9, 1, 3

# Model / training hyperparams
conv_filters = 16
conv_kh, conv_kw = 3, 1
encoder_lstm_units = 128
decoder_lstm_units = 128
dropout_rate = 0.3
l2_reg = 3e-4

batch_size = 4     # multi-step outputs can be large — start small and increase if memory allows
epochs = 60
ckpt_filepath = "best_model_seq2seq.h5"

# ---------------- Helper: generate samples ----------------
def generate_xy_from_files(file_list, window_size=window_size, stride=stride, out_steps=out_steps):
    """
    从 per-event npz 里生成滑窗样本（修正版）
    - 将标签与窗口**起点对齐**：y_seq = Y_event[start : start + out_steps]
    - 若未来不足 out_steps，会 pad 最后一帧（避免大量样本被跳过）
    返回:
      X_arr: (N, window_size, in_H, in_W, in_C)
      Y_arr: (N, out_steps, out_H, out_W, out_C)
    """
    Xs = []
    Ys = []
    skipped_short_input = 0
    skipped_short_output = 0
    for fp in file_list:
        data = np.load(fp, allow_pickle=True)
        X_event = data['X']  # shape (T_in, 3)
        Y_event = data['Y']  # shape (T_out, 9, 3)
        T = X_event.shape[0]
        T_out = Y_event.shape[0]

        if T < window_size:
            skipped_short_input += 1
            continue

        # max possible window start (inclusive)
        max_start = T - window_size
        for start in range(0, max_start + 1, stride):
            # input window (aligned to start)
            x_win = X_event[start: start + window_size]  # (window_size, 3)

            # label segment: align by window start (not window end)
            y_start = start
            # take available portion
            if y_start >= T_out:
                # no overlap with Y (should rarely happen) -> skip this window
                skipped_short_output += 1
                continue

            available = Y_event[y_start: min(T_out, y_start + out_steps)]
            if available.shape[0] < out_steps:
                # pad by repeating last available frame if exists, else pad with zeros
                if available.shape[0] == 0:
                    # fallback: use last frame of Y_event (if exists) else zeros
                    if T_out > 0:
                        pad_block = np.repeat(Y_event[-1:], out_steps, axis=0)
                    else:
                        pad_block = np.zeros((out_steps, out_H, out_W, out_C), dtype=np.float32)
                    y_seq = pad_block
                else:
                    pad_len = out_steps - available.shape[0]
                    last = available[-1:]
                    pad_block = np.repeat(last, pad_len, axis=0)
                    y_seq = np.concatenate([available, pad_block], axis=0)
            else:
                y_seq = available[:out_steps]

            # reshape to model expected dims
            x5d = x_win.reshape(window_size, in_H, in_W, in_C).astype(np.float32)   # (window_size,3,1,1)
            y5d = y_seq.reshape(out_steps, out_H, out_W, out_C).astype(np.float32)  # (out_steps,9,1,3)
            Xs.append(x5d)
            Ys.append(y5d)

    if not Xs:
        raise RuntimeError(
            f"未生成任何样本 (skipped_short_input={skipped_short_input}, skipped_short_output={skipped_short_output}). "
            f"请检查 processed_resampled_events 中的 X/Y 长度 或 调整 window/out_steps/stride。"
        )

    X_arr = np.stack(Xs, axis=0).astype(np.float32)
    Y_arr = np.stack(Ys, axis=0).astype(np.float32)
    print(f"Generated samples: X {X_arr.shape}, Y {Y_arr.shape}, skipped_short_input={skipped_short_input}, skipped_short_output={skipped_short_output}")
    return X_arr, Y_arr

# ---------------- Data loading & normalization ----------------
def load_and_prepare_data(processed_dir, val_split_ratio=0.10):
    files = sorted(glob.glob(os.path.join(processed_dir, "*.npz")))
    if not files:
        raise RuntimeError(f"{processed_dir} 中未找到 .npz 文件")

    # simple split by files: last val_split_ratio fraction as validation
    n_files = len(files)
    n_val = max(1, int(round(n_files * val_split_ratio)))
    train_files = files[:-n_val]
    val_files = files[-n_val:]
    print(f"Using {len(train_files)} train files and {len(val_files)} val files.")

    X_train, Y_train = generate_xy_from_files(train_files, window_size=window_size, stride=stride, out_steps=out_steps)
    X_val, Y_val = generate_xy_from_files(val_files, window_size=window_size, stride=stride, out_steps=out_steps)

    # compute per-spatial-channel mean/std over batch & time dims
    # X shape: (N, window_size, in_H, in_W, in_C) -> mean over axis (0,1)
    mean_X = X_train.mean(axis=(0, 1), keepdims=True)   # shape (1,1,in_H,in_W,in_C)
    std_X = X_train.std(axis=(0, 1), keepdims=True) + 1e-8
    X_train_s = (X_train - mean_X) / std_X
    X_val_s = (X_val - mean_X) / std_X

    # Y shape: (N, out_steps, out_H, out_W, out_C) -> mean over axis (0,1)
    mean_Y = Y_train.mean(axis=(0, 1), keepdims=True)   # shape (1,1,out_H,out_W,out_C)
    std_Y = Y_train.std(axis=(0, 1), keepdims=True) + 1e-8
    Y_train_s = (Y_train - mean_Y) / std_Y
    Y_val_s = (Y_val - mean_Y) / std_Y

    scalers = {'mean_X': mean_X, 'std_X': std_X, 'mean_Y': mean_Y, 'std_Y': std_Y}
    return X_train_s, Y_train_s, X_val_s, Y_val_s, scalers

# ---------------- Build model ----------------
def combined_loss(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred))

def build_and_compile_model(params):
    model = build_hybrid_conv_lstm_seq2seq(
        n_timesteps=window_size,
        in_H=in_H, in_W=in_W, in_C=in_C,
        out_steps=out_steps,
        out_H=out_H, out_W=out_W, out_C=out_C,
        conv_filters=int(params.get('conv_filters', conv_filters)),
        conv_kernel=(int(params.get('conv_kh', conv_kh)), int(params.get('conv_kw', conv_kw))),
        encoder_lstm_units=int(params.get('encoder_lstm_units', encoder_lstm_units)),
        decoder_lstm_units=int(params.get('decoder_lstm_units', decoder_lstm_units)),
        dropout_rate=float(params.get('dropout', dropout_rate)),
        l2_reg=float(params.get('l2_reg', l2_reg))
    )
    lr = float(params.get('lr', 1e-3))
    opt = Adam(learning_rate=lr)
    model.compile(optimizer=opt, loss=combined_loss, metrics=['mae'])
    return model

# ---------------- Training wrapper ----------------
def train_main():
    t0 = time.time()
    X_train, Y_train, X_val, Y_val, scalers = load_and_prepare_data(processed_dir, val_split_ratio=0.10)
    # Save scalers
    np.savez("scalers_seq2seq_resampled.npz", mean_X=scalers['mean_X'], std_X=scalers['std_X'],
             mean_Y=scalers['mean_Y'], std_Y=scalers['std_Y'], target_fs=target_fs)

    params = {
        'conv_filters': conv_filters,
        'conv_kh': conv_kh, 'conv_kw': conv_kw,
        'encoder_lstm_units': encoder_lstm_units, 'decoder_lstm_units': decoder_lstm_units,
        'dropout': dropout_rate, 'l2_reg': l2_reg, 'lr': 1e-3
    }

    model = build_and_compile_model(params)
    model.summary()

    callbacks = [
        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),
        tf.keras.callbacks.ModelCheckpoint(ckpt_filepath, monitor='val_loss', save_best_only=True, save_weights_only=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)
    ]

    history = model.fit(
        X_train, Y_train,
        validation_data=(X_val, Y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        shuffle=True
    )

    # save history and params
    with open("train_history_seq2seq.json", "w") as f:
        json.dump({k: [float(x) for x in v] for k, v in history.history.items()}, f)
    with open("best_params_seq2seq.json", "w") as f:
        json.dump(params, f, indent=2)

    model.save_weights(ckpt_filepath)
    print(f"Training finished. Weights saved to {ckpt_filepath}. Elapsed { (time.time()-t0)/3600:.2f} hours")

if __name__ == "__main__":
    train_main()
