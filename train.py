# train_resampled.py
"""
Multi-step training script for resampled dataset.
Assumes processed_resampled_events/*.npz exist, each containing:
  - X: (T_in, 3)  # axes order: z, y, x (as we prepared)
  - Y: (T_out, 9, 3)

This script generates (X_window, Y_future) training pairs:
  - X_window shape: (window_size, in_H, in_W, in_C)
  - Y_future shape: (out_steps, out_H, out_W, out_C)

Configuration near the top — change target_fs/window_seconds/out_seconds as needed.
"""
import os
import glob
import json
import pickle
import time
import numpy as np
import tensorflow as tf
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau

from model4 import build_hybrid_conv_lstm_seq2seq

# reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# ---------------- Config (modify if needed) ----------------
processed_dir = "./processed_resampled_events"   # generated by data_loader_resample.py
if not os.path.isdir(processed_dir):
    raise RuntimeError(f"请先运行 data_loader_resample.py，生成 {processed_dir}")

target_fs = 50.0          # must match preprocessing resample target
window_seconds = 16.0
out_seconds = 60.0

window_size = int(round(window_seconds * target_fs))   # e.g., 800
out_steps = int(round(out_seconds * target_fs))        # e.g., 3000
stride_seconds = 1.0
stride = max(1, int(round(stride_seconds * target_fs)))   # e.g., 50 -> slide by 1s, adjust as needed

# Input spatialization: treat 3 axes as spatial height
in_H, in_W, in_C = 3, 1, 1
out_H, out_W, out_C = 9, 1, 3

# Model / training hyperparams
conv_filters = 16
conv_kh, conv_kw = 3, 1
encoder_lstm_units = 128
decoder_lstm_units = 128
dropout_rate = 0.3
l2_reg = 3e-4

batch_size = 4     # multi-step outputs can be large — start small and increase if memory allows
epochs = 60
ckpt_filepath = "best_model_seq2seq.h5"

# ---------------- Helper: generate samples ----------------
def generate_xy_from_files(file_list, window_size=window_size, stride=stride, out_steps=out_steps):
    Xs = []
    Ys = []
    skipped_events = 0
    for fp in file_list:
        data = np.load(fp, allow_pickle=True)
        X_event = data['X'].astype(np.float32)  # (T_in, 3)
        Y_event = data['Y'].astype(np.float32)  # (T_out, 9, 3)
        T_in = X_event.shape[0]
        T_out = Y_event.shape[0]
        # We require that future segment [y_start : y_start + out_steps] fully exists.
        # y_start is the time index immediately after window end.
        max_start = T_in - window_size
        if max_start < 0:
            skipped_events += 1
            continue
        for start in range(0, max_start + 1, stride):
            y_start = start + window_size  # first predicted index in Y_event
            if y_start + out_steps > T_out:
                # not enough future length in Y to form full out_steps -> skip this sample
                continue
            # form input window and target future sequence
            x_win = X_event[start: start + window_size]    # (window_size, 3)
            y_seq = Y_event[y_start: y_start + out_steps]   # (out_steps, 9, 3)

            # reshape input to (window_size, in_H, in_W, in_C)
            x5d = x_win.reshape(window_size, in_H, in_W, in_C).astype(np.float32)
            # reshape y to (out_steps, out_H, out_W, out_C)
            y5d = y_seq.reshape(out_steps, out_H, out_W, out_C).astype(np.float32)

            Xs.append(x5d)
            Ys.append(y5d)

    if len(Xs) == 0:
        raise RuntimeError("未生成任何训练样本 — 请确认 processed_resampled_events 中的事件长度足够或调整 window/out_steps/stride 设置。")

    X_arr = np.stack(Xs, axis=0)   # (N, window_size, in_H, in_W, in_C)
    Y_arr = np.stack(Ys, axis=0)   # (N, out_steps, out_H, out_W, out_C)
    print(f"Generated samples: X {X_arr.shape}, Y {Y_arr.shape}, skipped_events {skipped_events}")
    return X_arr, Y_arr

# ---------------- Data loading & normalization ----------------
def load_and_prepare_data(processed_dir, val_split_ratio=0.10):
    files = sorted(glob.glob(os.path.join(processed_dir, "*.npz")))
    if not files:
        raise RuntimeError(f"{processed_dir} 中未找到 .npz 文件")

    # simple split by files: last val_split_ratio fraction as validation
    n_files = len(files)
    n_val = max(1, int(round(n_files * val_split_ratio)))
    train_files = files[:-n_val]
    val_files = files[-n_val:]
    print(f"Using {len(train_files)} train files and {len(val_files)} val files.")

    X_train, Y_train = generate_xy_from_files(train_files, window_size=window_size, stride=stride, out_steps=out_steps)
    X_val, Y_val = generate_xy_from_files(val_files, window_size=window_size, stride=stride, out_steps=out_steps)

    # compute per-spatial-channel mean/std over batch & time dims
    # X shape: (N, window_size, in_H, in_W, in_C) -> mean over axis (0,1)
    mean_X = X_train.mean(axis=(0, 1), keepdims=True)   # shape (1,1,in_H,in_W,in_C)
    std_X = X_train.std(axis=(0, 1), keepdims=True) + 1e-8
    X_train_s = (X_train - mean_X) / std_X
    X_val_s = (X_val - mean_X) / std_X

    # Y shape: (N, out_steps, out_H, out_W, out_C) -> mean over axis (0,1)
    mean_Y = Y_train.mean(axis=(0, 1), keepdims=True)   # shape (1,1,out_H,out_W,out_C)
    std_Y = Y_train.std(axis=(0, 1), keepdims=True) + 1e-8
    Y_train_s = (Y_train - mean_Y) / std_Y
    Y_val_s = (Y_val - mean_Y) / std_Y

    scalers = {'mean_X': mean_X, 'std_X': std_X, 'mean_Y': mean_Y, 'std_Y': std_Y}
    return X_train_s, Y_train_s, X_val_s, Y_val_s, scalers

# ---------------- Build model ----------------
def combined_loss(y_true, y_pred):
    return tf.reduce_mean(tf.square(y_true - y_pred))

def build_and_compile_model(params):
    model = build_hybrid_conv_lstm_seq2seq(
        n_timesteps=window_size,
        in_H=in_H, in_W=in_W, in_C=in_C,
        out_steps=out_steps,
        out_H=out_H, out_W=out_W, out_C=out_C,
        conv_filters=int(params.get('conv_filters', conv_filters)),
        conv_kernel=(int(params.get('conv_kh', conv_kh)), int(params.get('conv_kw', conv_kw))),
        encoder_lstm_units=int(params.get('encoder_lstm_units', encoder_lstm_units)),
        decoder_lstm_units=int(params.get('decoder_lstm_units', decoder_lstm_units)),
        dropout_rate=float(params.get('dropout', dropout_rate)),
        l2_reg=float(params.get('l2_reg', l2_reg))
    )
    lr = float(params.get('lr', 1e-3))
    opt = Adam(learning_rate=lr)
    model.compile(optimizer=opt, loss=combined_loss, metrics=['mae'])
    return model

# ---------------- Training wrapper ----------------
def train_main():
    t0 = time.time()
    X_train, Y_train, X_val, Y_val, scalers = load_and_prepare_data(processed_dir, val_split_ratio=0.10)
    # Save scalers
    np.savez("scalers_seq2seq_resampled.npz", mean_X=scalers['mean_X'], std_X=scalers['std_X'],
             mean_Y=scalers['mean_Y'], std_Y=scalers['std_Y'], target_fs=target_fs)

    params = {
        'conv_filters': conv_filters,
        'conv_kh': conv_kh, 'conv_kw': conv_kw,
        'encoder_lstm_units': encoder_lstm_units, 'decoder_lstm_units': decoder_lstm_units,
        'dropout': dropout_rate, 'l2_reg': l2_reg, 'lr': 1e-3
    }

    model = build_and_compile_model(params)
    model.summary()

    callbacks = [
        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True),
        tf.keras.callbacks.ModelCheckpoint(ckpt_filepath, monitor='val_loss', save_best_only=True, save_weights_only=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-6)
    ]

    history = model.fit(
        X_train, Y_train,
        validation_data=(X_val, Y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=callbacks,
        shuffle=True
    )

    # save history and params
    with open("train_history_seq2seq.json", "w") as f:
        json.dump({k: [float(x) for x in v] for k, v in history.history.items()}, f)
    with open("best_params_seq2seq.json", "w") as f:
        json.dump(params, f, indent=2)

    model.save_weights(ckpt_filepath)
    print(f"Training finished. Weights saved to {ckpt_filepath}. Elapsed { (time.time()-t0)/3600:.2f} hours")

if __name__ == "__main__":
    train_main()
